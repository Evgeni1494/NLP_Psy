{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NodeConfig.__init__() missing 1 required positional argument: 'scheme'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfaker\u001b[39;00m \u001b[39mimport\u001b[39;00m Faker\n\u001b[1;32m      4\u001b[0m \u001b[39m# Établir la connexion à Elasticsearch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m es \u001b[39m=\u001b[39m Elasticsearch([{\u001b[39m'\u001b[39;49m\u001b[39mhost\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mport\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m9200\u001b[39;49m}])\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:331\u001b[0m, in \u001b[0;36mElasticsearch.__init__\u001b[0;34m(self, hosts, cloud_id, api_key, basic_auth, bearer_auth, opaque_id, headers, connections_per_node, http_compress, verify_certs, ca_certs, client_cert, client_key, ssl_assert_hostname, ssl_assert_fingerprint, ssl_version, ssl_context, ssl_show_warn, transport_class, request_timeout, node_class, node_pool_class, randomize_nodes_in_pool, node_selector_class, dead_node_backoff_factor, max_dead_node_backoff, serializer, serializers, default_mimetype, max_retries, retry_on_status, retry_on_timeout, sniff_on_start, sniff_before_requests, sniff_on_node_failure, sniff_timeout, min_delay_between_sniffing, sniffed_node_callback, meta_header, timeout, randomize_hosts, host_info_callback, sniffer_timeout, sniff_on_connection_fail, http_auth, maxsize, _transport)\u001b[0m\n\u001b[1;32m    328\u001b[0m         requests_session_auth \u001b[39m=\u001b[39m http_auth\n\u001b[1;32m    329\u001b[0m         http_auth \u001b[39m=\u001b[39m DEFAULT\n\u001b[0;32m--> 331\u001b[0m node_configs \u001b[39m=\u001b[39m client_node_configs(\n\u001b[1;32m    332\u001b[0m     hosts,\n\u001b[1;32m    333\u001b[0m     cloud_id\u001b[39m=\u001b[39;49mcloud_id,\n\u001b[1;32m    334\u001b[0m     requests_session_auth\u001b[39m=\u001b[39;49mrequests_session_auth,\n\u001b[1;32m    335\u001b[0m     connections_per_node\u001b[39m=\u001b[39;49mconnections_per_node,\n\u001b[1;32m    336\u001b[0m     http_compress\u001b[39m=\u001b[39;49mhttp_compress,\n\u001b[1;32m    337\u001b[0m     verify_certs\u001b[39m=\u001b[39;49mverify_certs,\n\u001b[1;32m    338\u001b[0m     ca_certs\u001b[39m=\u001b[39;49mca_certs,\n\u001b[1;32m    339\u001b[0m     client_cert\u001b[39m=\u001b[39;49mclient_cert,\n\u001b[1;32m    340\u001b[0m     client_key\u001b[39m=\u001b[39;49mclient_key,\n\u001b[1;32m    341\u001b[0m     ssl_assert_hostname\u001b[39m=\u001b[39;49mssl_assert_hostname,\n\u001b[1;32m    342\u001b[0m     ssl_assert_fingerprint\u001b[39m=\u001b[39;49mssl_assert_fingerprint,\n\u001b[1;32m    343\u001b[0m     ssl_version\u001b[39m=\u001b[39;49mssl_version,\n\u001b[1;32m    344\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mssl_context,\n\u001b[1;32m    345\u001b[0m     ssl_show_warn\u001b[39m=\u001b[39;49mssl_show_warn,\n\u001b[1;32m    346\u001b[0m )\n\u001b[1;32m    347\u001b[0m transport_kwargs: t\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, t\u001b[39m.\u001b[39mAny] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m node_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m DEFAULT:\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:105\u001b[0m, in \u001b[0;36mclient_node_configs\u001b[0;34m(hosts, cloud_id, requests_session_auth, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39massert\u001b[39;00m hosts \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     node_configs \u001b[39m=\u001b[39m hosts_to_node_configs(hosts)\n\u001b[1;32m    107\u001b[0m \u001b[39m# Remove all values which are 'DEFAULT' to avoid overwriting actual defaults.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m node_options \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m DEFAULT}\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:154\u001b[0m, in \u001b[0;36mhosts_to_node_configs\u001b[0;34m(hosts)\u001b[0m\n\u001b[1;32m    151\u001b[0m     node_configs\u001b[39m.\u001b[39mappend(url_to_node_config(host))\n\u001b[1;32m    153\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(host, Mapping):\n\u001b[0;32m--> 154\u001b[0m     node_configs\u001b[39m.\u001b[39mappend(host_mapping_to_node_config(host))\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhosts\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a list of URLs, NodeConfigs, or dictionaries\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:221\u001b[0m, in \u001b[0;36mhost_mapping_to_node_config\u001b[0;34m(host)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39murl_prefix\u001b[39m\u001b[39m'\u001b[39m\u001b[39m option is deprecated in favor of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath_prefix\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    217\u001b[0m         stacklevel\u001b[39m=\u001b[39mwarn_stacklevel(),\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m     options[\u001b[39m\"\u001b[39m\u001b[39mpath_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39murl_prefix\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m NodeConfig(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "\u001b[0;31mTypeError\u001b[0m: NodeConfig.__init__() missing 1 required positional argument: 'scheme'"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from faker import Faker\n",
    "\n",
    "# Établir la connexion à Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('emotion.csv')\n",
    "\n",
    "\n",
    "df1=df.iloc[:8000]\n",
    "df2=df.iloc[8000:16000]\n",
    "df3=df.iloc[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21459, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m fake \u001b[39m=\u001b[39m Faker()\n\u001b[1;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpic_nlp.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> 21\u001b[0m     modele_ml \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Nombre de documents à générer\u001b[39;00m\n\u001b[1;32m     23\u001b[0m liste_nom\u001b[39m=\u001b[39m[]\n",
      "File \u001b[0;32m~/Documents/elastic/preprocessing.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39mpunkt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/nltk/downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(s, prefix2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    769\u001b[0m     print_to(\n\u001b[1;32m    770\u001b[0m         textwrap\u001b[39m.\u001b[39mfill(\n\u001b[1;32m    771\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 777\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[1;32m    778\u001b[0m     \u001b[39m# Error messages\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[1;32m    780\u001b[0m         show(msg\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/nltk/downloader.py:629\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m# Look up the requested collection or package.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_or_id(info_or_id)\n\u001b[1;32m    630\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    631\u001b[0m     \u001b[39myield\u001b[39;00m ErrorMessage(\u001b[39mNone\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading \u001b[39m\u001b[39m{\u001b[39;00minfo_or_id\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/nltk/downloader.py:603\u001b[0m, in \u001b[0;36mDownloader._info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_info_or_id\u001b[39m(\u001b[39mself\u001b[39m, info_or_id):\n\u001b[1;32m    602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info_or_id, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 603\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo(info_or_id)\n\u001b[1;32m    604\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m         \u001b[39mreturn\u001b[39;00m info_or_id\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/nltk/downloader.py:1009\u001b[0m, in \u001b[0;36mDownloader.info\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfo\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mid\u001b[39m):\n\u001b[1;32m   1007\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the ``Package`` or ``Collection`` record for the\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[39m    given item.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_index()\n\u001b[1;32m   1010\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packages:\n\u001b[1;32m   1011\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_packages[\u001b[39mid\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/nltk/downloader.py:952\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url \u001b[39m=\u001b[39m url \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\n\u001b[1;32m    950\u001b[0m \u001b[39m# Download the index file.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mElementWrapper(\n\u001b[0;32m--> 952\u001b[0m     ElementTree\u001b[39m.\u001b[39;49mparse(urlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url))\u001b[39m.\u001b[39mgetroot()\n\u001b[1;32m    953\u001b[0m )\n\u001b[1;32m    954\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_timestamp \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    956\u001b[0m \u001b[39m# Build a dictionary of packages.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \n\u001b[1;32m   1215\u001b[0m \u001b[39m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m tree \u001b[39m=\u001b[39m ElementTree()\n\u001b[0;32m-> 1222\u001b[0m tree\u001b[39m.\u001b[39;49mparse(source, parser)\n\u001b[1;32m   1223\u001b[0m \u001b[39mreturn\u001b[39;00m tree\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/xml/etree/ElementTree.py:580\u001b[0m, in \u001b[0;36mElementTree.parse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    574\u001b[0m     parser \u001b[39m=\u001b[39m XMLParser()\n\u001b[1;32m    575\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(parser, \u001b[39m'\u001b[39m\u001b[39m_parse_whole\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    576\u001b[0m         \u001b[39m# The default XMLParser, when it comes from an accelerator,\u001b[39;00m\n\u001b[1;32m    577\u001b[0m         \u001b[39m# can define an internal _parse_whole API for efficiency.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m         \u001b[39m# It can be used to parse the whole source without feeding\u001b[39;00m\n\u001b[1;32m    579\u001b[0m         \u001b[39m# it with chunks.\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_root \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49m_parse_whole(source)\n\u001b[1;32m    581\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_root\n\u001b[1;32m    582\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from faker import Faker\n",
    "import pickle\n",
    "# Établir la connexion à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# Nom de l'index\n",
    "index_name = 'notes'\n",
    "\n",
    "# Créer une instance de Fakercurl -X POST 'http://localhost:9200/articles/_doc?pretty' -H 'Content-Type: application/json' -d '\n",
    "{\n",
    "    \"created_at\": \"2021-06-07T16:24:32.000Z\",\n",
    "    \"title\": \"mon article sur Elasticsearch\",\n",
    "    \"content\": \"ceci est le contenu de mon article sur Elasticsearch\",\n",
    "    \"category\": \"elasticsearch\",\n",
    "    \"author\": \"Hatim\"\n",
    "  }\n",
    "fake = Faker()\n",
    "\n",
    "with open('pic_nlp.pkl', 'rb') as file:\n",
    "    modele_ml = pickle.load(file)\n",
    "# Nombre de documents à générer\n",
    "liste_nom=[]\n",
    "liste_prenom=[]\n",
    "\n",
    "count = 0\n",
    "rang= 0\n",
    "for i in range (500):\n",
    "    nom = fake.last_name()\n",
    "    prenom = fake.first_name()\n",
    "    liste_nom.append(nom)\n",
    "    liste_prenom.append(prenom)\n",
    "    \n",
    "for index, row in df1.iterrows():\n",
    "    print(index)\n",
    "    if index %50==0:\n",
    "        rang+=1\n",
    "    document = {\n",
    "        'patient_lastname': liste_nom[rang],\n",
    "        'patient_firstname': liste_prenom[rang],\n",
    "        'text': row['Text'],\n",
    "        'date': fake.date_object(),\n",
    "        'patient_left': fake.boolean(),\n",
    "        'emotion': row['Emotion'],\n",
    "        'confidence': fake.pyfloat()\n",
    "    }\n",
    "    \n",
    "    # Indexer le document dans Elasticsearch\n",
    "    response = es.index(index=index_name, body=document)\n",
    "    \n",
    "    if response['result'] == 'created':\n",
    "        print('Document indexé avec succès.')\n",
    "    else:\n",
    "        print(\"Erreur lors de l'indexation du document.\")\n",
    "   \n",
    "    count += 1\n",
    "\n",
    "es.indices.refresh(index=index_name)\n",
    "es.transport.close()\n",
    "\n",
    "print(f\"Indexation terminée pour documents.\")\n",
    "\n",
    "\n",
    "# commande pour voir ts les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GET /'notes'/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"notes\": \"important\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36568/2443269931.py:11: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  mapping = es.indices.get_mapping(index=index_name)\n",
      "/tmp/ipykernel_36568/2443269931.py:27: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  result = es.search(index=index_name, body=requete)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'search_phase_execution_exception', 'Result window is too large, from + size must be less than or equal to: [10000] but was [20000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m requete \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmatch_all\u001b[39m\u001b[39m\"\u001b[39m: {}\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m10000\u001b[39m\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m \u001b[39m# Exécuter la requête de recherche\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39;49msearch(index\u001b[39m=\u001b[39;49mindex_name, body\u001b[39m=\u001b[39;49mrequete)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Extraire les données des documents\u001b[39;00m\n\u001b[1;32m     30\u001b[0m documents \u001b[39m=\u001b[39m [hit[\u001b[39m'\u001b[39m\u001b[39m_source\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m hit \u001b[39min\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:3859\u001b[0m, in \u001b[0;36mElasticsearch.search\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, from_, highlight, human, ignore_throttled, ignore_unavailable, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, request_cache, rescore, rest_total_hits_as_int, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version)\u001b[0m\n\u001b[1;32m   3857\u001b[0m \u001b[39mif\u001b[39;00m __body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3858\u001b[0m     __headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 3859\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   3860\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers, body\u001b[39m=\u001b[39;49m__body\n\u001b[1;32m   3861\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:320\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    318\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[39m.\u001b[39mget(meta\u001b[39m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    321\u001b[0m         message\u001b[39m=\u001b[39mmessage, meta\u001b[39m=\u001b[39mmeta, body\u001b[39m=\u001b[39mresp_body\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[39m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    326\u001b[0m     \u001b[39m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'search_phase_execution_exception', 'Result window is too large, from + size must be less than or equal to: [10000] but was [20000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level setting.')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "# Se connecter à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# Nom de l'index à récupérer les données\n",
    "index_name = 'notes'\n",
    "\n",
    "# Récupérer les informations de mappage de l'index\n",
    "mapping = es.indices.get_mapping(index=index_name)\n",
    "properties = mapping[index_name]['mappings']['properties']\n",
    "\n",
    "# Extraire les noms des champs du mappage\n",
    "champs = list(properties.keys())\n",
    "\n",
    "# Définir la requête de recherche pour récupérer toutes les lignes de l'index\n",
    "requete = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    \"size\": 10000,  # Nombre de résultats à récupérer\n",
    "    \"from\":10000\n",
    "}\n",
    "\n",
    "# Exécuter la requête de recherche\n",
    "result = es.search(index=index_name, body=requete)\n",
    "\n",
    "# Extraire les données des documents\n",
    "documents = [hit['_source'] for hit in result['hits']['hits']]\n",
    "\n",
    "# Créer un DataFrame à partir des données\n",
    "df1 = pd.DataFrame(documents, columns=champs)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36568/3223417957.py:11: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  mapping = es.indices.get_mapping(index=index_name)\n",
      "/tmp/ipykernel_36568/3223417957.py:27: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  result = es.search(index=index_name, body=requete, scroll=\"1m\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received multiple values for 'scroll', specify parameters directly instead of using 'body' or 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m requete \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmatch_all\u001b[39m\u001b[39m\"\u001b[39m: {}\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscroll\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1m\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Durée de validité du scroll\u001b[39;00m\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m \u001b[39m# Exécuter la requête de recherche initiale\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39;49msearch(index\u001b[39m=\u001b[39;49mindex_name, body\u001b[39m=\u001b[39;49mrequete, scroll\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1m\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Extraire les données des documents\u001b[39;00m\n\u001b[1;32m     30\u001b[0m documents \u001b[39m=\u001b[39m [hit[\u001b[39m'\u001b[39m\u001b[39m_source\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m hit \u001b[39min\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:405\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt merge \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m\u001b[39m with other parameters as it wasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a mapping. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mInstead of using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m\u001b[39m use individual API parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m                 )\n\u001b[1;32m    399\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    400\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter is deprecated and will be removed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39min a future version. Instead use individual parameters.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m                 category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    403\u001b[0m                 stacklevel\u001b[39m=\u001b[39mwarn_stacklevel(),\n\u001b[1;32m    404\u001b[0m             )\n\u001b[0;32m--> 405\u001b[0m             _merge_kwargs_no_duplicates(kwargs, body)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m parameter_aliases:\n\u001b[1;32m    408\u001b[0m     \u001b[39mfor\u001b[39;00m alias, rename_to \u001b[39min\u001b[39;00m parameter_aliases\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:287\u001b[0m, in \u001b[0;36m_merge_kwargs_no_duplicates\u001b[0;34m(kwargs, values)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m values\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m--> 287\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived multiple values for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, specify parameters \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdirectly instead of using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m     kwargs[key] \u001b[39m=\u001b[39m val\n",
      "\u001b[0;31mValueError\u001b[0m: Received multiple values for 'scroll', specify parameters directly instead of using 'body' or 'params'"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "# Se connecter à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# Nom de l'index à récupérer les données\n",
    "index_name = 'notes'\n",
    "\n",
    "# Récupérer les informations de mappage de l'index\n",
    "mapping = es.indices.get_mapping(index=index_name)\n",
    "properties = mapping[index_name]['mappings']['properties']\n",
    "\n",
    "# Extraire les noms des champs du mappage\n",
    "champs = list(properties.keys())\n",
    "\n",
    "# Définir la requête de recherche initiale avec scroll\n",
    "requete = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    \"size\": 1000,  # Taille de chaque lot\n",
    "    \"scroll\": \"1m\"  # Durée de validité du scroll\n",
    "}\n",
    "\n",
    "# Exécuter la requête de recherche initiale\n",
    "result = es.search(index=index_name, body=requete, scroll=\"1m\")\n",
    "\n",
    "# Extraire les données des documents\n",
    "documents = [hit['_source'] for hit in result['hits']['hits']]\n",
    "\n",
    "# Tant qu'il y a plus de résultats, continuer à récupérer les lots suivants\n",
    "scroll_id = result['_scroll_id']\n",
    "while True:\n",
    "    # Récupérer le lot suivant avec scroll\n",
    "    result = es.scroll(scroll_id=scroll_id, scroll='1m')\n",
    "    scroll_id = result['_scroll_id']\n",
    "    \n",
    "    # Extraire les données des documents du lot suivant\n",
    "    documents.extend([hit['_source'] for hit in result['hits']['hits']])\n",
    "    \n",
    "    # Vérifier si tous les résultats ont été récupérés\n",
    "    if len(result['hits']['hits']) == 0:\n",
    "        break\n",
    "\n",
    "# Créer un DataFrame à partir des données\n",
    "df = pd.DataFrame(documents, columns=champs)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36568/2688201981.py:35: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  result = es.search(index=index_name, body=requete)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'search_phase_execution_exception', 'runtime error')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 35\u001b[0m\n\u001b[1;32m     10\u001b[0m requete \u001b[39m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m10\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maggs\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     }\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m \u001b[39m# Exécuter la requête\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m result \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39;49msearch(index\u001b[39m=\u001b[39;49mindex_name, body\u001b[39m=\u001b[39;49mrequete)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Récupérer les résultats de l'agrégation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m aggregation \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39maggregations\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpatients_uniques\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbuckets\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:3859\u001b[0m, in \u001b[0;36mElasticsearch.search\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, from_, highlight, human, ignore_throttled, ignore_unavailable, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, request_cache, rescore, rest_total_hits_as_int, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version)\u001b[0m\n\u001b[1;32m   3857\u001b[0m \u001b[39mif\u001b[39;00m __body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3858\u001b[0m     __headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 3859\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   3860\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers, body\u001b[39m=\u001b[39;49m__body\n\u001b[1;32m   3861\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:320\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    318\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[39m.\u001b[39mget(meta\u001b[39m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    321\u001b[0m         message\u001b[39m=\u001b[39mmessage, meta\u001b[39m=\u001b[39mmeta, body\u001b[39m=\u001b[39mresp_body\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[39m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    326\u001b[0m     \u001b[39m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'search_phase_execution_exception', 'runtime error')"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Se connecter à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# Index à interroger\n",
    "index_name = 'notes'\n",
    "\n",
    "# Requête d'agrégation\n",
    "requete = {\n",
    "    \"size\": 10,\n",
    "    \"aggs\": {\n",
    "        \"patients_uniques\": {\n",
    "            \"terms\": {\n",
    "                \"script\": {\n",
    "                    \"source\": \"doc['lastname.keyword'].value + ' ' + doc['surname.keyword'].value\",\n",
    "                    \"lang\": \"painless\"\n",
    "                },\n",
    "                \"size\": 10000\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"emotions_uniques\": {\n",
    "                    \"significant_terms\": {\n",
    "                        \"field\": \"emotion\",\n",
    "                        \"percentage\": {},\n",
    "                        \"size\": 10000\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Exécuter la requête\n",
    "result = es.search(index=index_name, body=requete)\n",
    "\n",
    "# Récupérer les résultats de l'agrégation\n",
    "aggregation = result['aggregations']['patients_uniques']['buckets']\n",
    "\n",
    "# Parcourir les résultats\n",
    "for bucket in aggregation:\n",
    "    patient = bucket['key']\n",
    "    emotions = bucket['emotions_uniques']['buckets']\n",
    "    total_occurrences = sum([bucket['doc_count'] for bucket in emotions])\n",
    "    \n",
    "    print(f\"Patient: {patient}\")\n",
    "    for emotion_bucket in emotions:\n",
    "        emotion = emotion_bucket['key']\n",
    "        occurrence_percentage = (emotion_bucket['doc_count'] / total_occurrences) * 100\n",
    "        print(f\"Emotion: {emotion}, Pourcentage: {occurrence_percentage:.2f}%\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour des champs 'confidence' terminée.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36568/3062926506.py:16: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es.search(index=index_name, body={\"query\": {\"match_all\": {}}}, size=9000)\n",
      "/tmp/ipykernel_36568/3062926506.py:16: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  response = es.search(index=index_name, body={\"query\": {\"match_all\": {}}}, size=9000)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Établir la connexion à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# Nom de l'index\n",
    "index_name = 'notes'\n",
    "\n",
    "# Charger le modèle ML à partir du fichier pickle\n",
    "with open('pic_nlp.pkl', 'rb') as file:\n",
    "    modele_ml = pickle.load(file)\n",
    "\n",
    "# Récupérer tous les documents de l'index \"notes\"\n",
    "response = es.search(index=index_name, body={\"query\": {\"match_all\": {}}}, size=9000)\n",
    "\n",
    "# Parcourir les résultats et mettre à jour le champ \"confidence\"\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit)\n",
    "    document_id = hit['_id']\n",
    "    document_text = hit['_source']['text']\n",
    "    doc_emotion=hit['_source']['emotion']\n",
    "    conf=hit['_source']['confidence']\n",
    "    print(conf)\n",
    "    # Appliquer le modèle ML pour obtenir le résultat\n",
    "    emotion = modele_ml.predict(doc_emotion)\n",
    "    decision_scores = modele_ml.decision_function([document_text])[0]\n",
    "\n",
    "# Transformer les scores de décision en probabilités normalisées\n",
    "    probabilities = decision_scores - decision_scores.min()\n",
    "    probabilities /= probabilities.max()\n",
    "\n",
    "# Obtenir le pourcentage de confiance en la prédiction\n",
    "    confidence_rslt = probabilities[np.argmax(probabilities)] * 100\n",
    "    \n",
    "    # Mettre à jour le champ \"confidence\" du document\n",
    "    update_body = {\"doc\": {\"emotion\": emotion, \"confidence\":confidence_rslt}}\n",
    "    es.update(index=index_name, id=document_id, body=update_body)\n",
    "\n",
    "print(\"Mise à jour des champs 'confidence' terminée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tout supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5890/1498190664.py:6: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  response = es.indices.delete(index=index_name, ignore=[400, 404])\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f2fe1dc4d30>: Failed to establish a new connection: [Errno 111] Connection refused))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# # # # Suppression de l'index \"notes\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnotes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response \u001b[39m=\u001b[39m es\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mdelete(index\u001b[39m=\u001b[39;49mindex_name, ignore\u001b[39m=\u001b[39;49m[\u001b[39m400\u001b[39;49m, \u001b[39m404\u001b[39;49m])\n\u001b[1;32m      8\u001b[0m \u001b[39m# # # # Vérification de la suppression\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39macknowledged\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py:690\u001b[0m, in \u001b[0;36mIndicesClient.delete\u001b[0;34m(self, index, allow_no_indices, error_trace, expand_wildcards, filter_path, human, ignore_unavailable, master_timeout, pretty, timeout)\u001b[0m\n\u001b[1;32m    688\u001b[0m     __query[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m timeout\n\u001b[1;32m    689\u001b[0m __headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 690\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mDELETE\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers\n\u001b[1;32m    692\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    390\u001b[0m         method, path, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, body\u001b[39m=\u001b[39;49mbody\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:285\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     target \u001b[39m=\u001b[39m path\n\u001b[0;32m--> 285\u001b[0m meta, resp_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    286\u001b[0m     method,\n\u001b[1;32m    287\u001b[0m     target,\n\u001b[1;32m    288\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    289\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    290\u001b[0m     request_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_timeout,\n\u001b[1;32m    291\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_retries,\n\u001b[1;32m    292\u001b[0m     retry_on_status\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_status,\n\u001b[1;32m    293\u001b[0m     retry_on_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_timeout,\n\u001b[1;32m    294\u001b[0m     client_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client_meta,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[39m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m meta\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    300\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m meta\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m \u001b[39m299\u001b[39m\n\u001b[1;32m    301\u001b[0m     \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elastic_transport/_transport.py:329\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta)\u001b[0m\n\u001b[1;32m    327\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    328\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     meta, raw_data \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    330\u001b[0m         method,\n\u001b[1;32m    331\u001b[0m         target,\n\u001b[1;32m    332\u001b[0m         body\u001b[39m=\u001b[39;49mrequest_body,\n\u001b[1;32m    333\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    334\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m     _logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m [status:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m duration:\u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39ms]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/machinelearning/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py:199\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[0;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[1;32m    191\u001b[0m         err \u001b[39m=\u001b[39m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mstr\u001b[39m(e), errors\u001b[39m=\u001b[39m(e,))\n\u001b[1;32m    192\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(\n\u001b[1;32m    193\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    194\u001b[0m         target\u001b[39m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m         exception\u001b[39m=\u001b[39merr,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mraise\u001b[39;00m err \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    201\u001b[0m meta \u001b[39m=\u001b[39m ApiResponseMeta(\n\u001b[1;32m    202\u001b[0m     node\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m    203\u001b[0m     duration\u001b[39m=\u001b[39mduration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     headers\u001b[39m=\u001b[39mresponse_headers,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(\n\u001b[1;32m    209\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    210\u001b[0m     target\u001b[39m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     response\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    215\u001b[0m )\n",
      "\u001b[0;31mConnectionError\u001b[0m: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7f2fe1dc4d30>: Failed to establish a new connection: [Errno 111] Connection refused))"
     ]
    }
   ],
   "source": [
    "# #  # # Connexion à Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "\n",
    "# # # # Suppression de l'index \"notes\"\n",
    "index_name = \"notes\"\n",
    "response = es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "\n",
    "# # # # Vérification de la suppression\n",
    "if response[\"acknowledged\"]:\n",
    "    print(f\"L'index '{index_name}' a été supprimé avec succès.\")\n",
    "else:\n",
    "    print(f\"La suppression de l'index '{index_name}' a échoué.\")\n",
    "\n",
    "# # # # Fermeture de la connexion à Elasticsearch\n",
    "es.transport.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])es = Elasticsearch(hosts=[\"http://localhost:9200\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abfbbb82e5734ddf6fe9d05af3064de704bc0f7110367d6a6ae63e383d8540d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
